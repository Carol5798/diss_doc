\chapter{System Requirements, Specification, Design and Implementation}\label{ch:specs_design_implem}

This chapter presents the proposed solution, encompassing system requirements, specification, design and implementation.
Section~\ref{sec:specs} discusses the requirements and specifications, Section~\ref{sec:prop_vision} presents the proposed vision module.
Section~\ref{sec:design} introduces the design.
Finally , Section~\ref{sec:impl} presents the implementation of the system.

\section{System Requirements and Specification}\label{sec:specs}
The goal of this dissertation was to extract vision-based information relevant to a 5G network.
This information should be available in near real-time to relevant entities of the network architecture upon subscription.
This solution envisions obstacle-aware networks and should enable a Base Station to autonomously control its placement and configuration based on the environment perception provided by the vision-based information.
To achieve this goal, specific system requirements were established to ensure the efficient detection, tracking, and communication of obstacles within the 5G network.

The detailed requirements of the Computer Vision Module and its interface with the network are as follows:


% check this requirements with the results
\begin{enumerate}
    \item \textbf{Functional Requirements}:
    \begin{enumerate}
        \item Detect and track objects in near real-time using computer vision algorithms.
        \item Identify specific obstacles and predict potential blockages.
        \item Send formatted messages to the xApp for further processing and decision-making.
        \item Ensure interoperability with the 5G network components via standardized messaging protocols. % should i use standarized, since there is no standard for it, i created a structure?
    \end{enumerate}
    \item \textbf{Non-Functional Requirements}:
    \begin{enumerate}
        \item Processing speed: Must process video frames within a maximum of 100ms.
        \item Messaging latency: Messages must be sent within 50ms of detection.
        \item Accuracy: High accuracy in object detection and tracking to minimize false positives/negatives.
        \item Scalability: The solution should be scalable to handle multiple video streams, Base Stations and UEs, allowing for broader deployment in various network environments.
        \item Interoperability: The system must use standardized messaging formats to ensure interoperability with different network components and vendors.
        \item Reliability: The vision module and communication system should be robust, with mechanisms for error detection and recovery to maintain continuous operation. The model and its parameters are fundamental to assure reliability. % how can i prove it?
    \end{enumerate}
\end{enumerate}
\textcolor{red}{Devo remover os requerimentos não funcionais que não foram cumpridos, ou justifico o porque de não ser possível cumprí-los?}

The requirements translate into table ~\ref{tab:spec}, containing system specifications:

\begin{table}[H]
\caption{Specifications}
\label{tab:spec}
\centering
\resizebox{\textwidth}{!}{%
\begin{tabular}{|l|p{12cm}|}
\hline
\textbf{Specification} & \textbf{Description} \\ \hline
\textbf{Detection and Tracking} & Use YOLOv5 Ultralytics and BoT-SORT for high accuracy and optimized speed. \\ \hline
\textbf{Messaging} & Encode messages using ASN.1 standards. \newline Transmit messages via SCTP protocol. \newline Include relevant information such as object ID, type, position (x, y coordinates), and confidence score. \\ \hline
\end{tabular}}
\end{table}

The above specifications ensure that the Vision Module meets the requirements necessary for integration and operation within a 5G network environment.


Following the requirements and specifications, the system architecture was designed to facilitate efficient data processing and communication:

\begin{figure}[H]
    \centering
    \includegraphics[width=0.7\linewidth]{figures/Syst_Arch.drawio}
    \caption[System Architecture of the proposed solution]{System Architecture of the proposed solution}
    \label{fig:my_arch}
\end{figure}

% draw: SMF[x], SPGWU[x], AMF[x], AUSF, UDM, UDR, mysql, NRF ext DN[x]
% requires improvements
In the proposed architecture, shown in Figure~\ref{fig:my_arch}, a UE requires 5G connectivity.
The 5G connection workflow begins with the UE initiating a connection request to the gNB, which then forwards the request to the AMF\@.
The gNB serves as the intermediary between the UE and the 5G core network, managing the radio resources, handling data transmission, and ensuring seamless connectivity as the UE moves.
The AMF authenticates the UE using credentials stored in the UDM. Upon successful authentication, the UE is granted access to the network.
The AMF and UPF then establish a data session for the UE, configuring the necessary resources for data transmission.
User data is transmitted between the UE and the UPF via the gNB\@.
The UPF routes the data to and from external networks, ensuring efficient data flow.

%------------ move to another place
The 5G Core Network is responsible for managing the overall network functions, including authentication, mobility management, and routing of data.
In our solution, both the RAN and the core are deployed within the same processing unit to maintain efficiency, mobility and simplicity.
The AMF is a critical component of the 5G core that handles user authentication and mobility management. When a UE attempts to connect to the network, the AMF verifies the user's credentials and manages their session as they move across different cells in the network.

The User Plane Function (UPF) manages the user data traffic, ensuring that data packets are efficiently routed between the UE and external data networks. It plays a pivotal role in delivering low-latency, high-bandwidth services to the end-users. Another essential core network function is the Unified Data Management (UDM), responsible for handling subscriber data and profiles. It ensures that user data is consistent and accessible across the network, supporting seamless user experiences.

To ensure the mobility and efficiency of our solution, the RAN and the core network components are deployed within the same processing unit.
This integration offers several advantages. By colocating the RAN and core network functions, data processing and transmission delays are minimized, resulting in a more responsive network. The mobile RAN can seamlessly maintain connectivity with the UE, even when the gNB is moving, without relying on distant core network infrastructure. Having the RAN and core in a single unit simplifies the deployment process, making it easier to set up and manage the network in various locations, whether stationary or on the move.
%-------------------

In our solution, the RAN can be repositioned according to environment conditions that possibly affect the RF\@.
Those are reported by the Vision Module (Vision Module).
This enables the RAN to control manage its placement, based on those conditions together with radio metrics, such as SNR\@.
In order to establish the integration of Computer Vision into the 5G architecture, we took advantage of the Near-RT RIC, specified by O-RAN, to deploy a xApp responsible for the handling both radio metrics and the VM messages.
The communication between the VM and the xApp, is done through an interface inspired by the O-RAN E2 interface and E2 Application Protocol (E2AP).
This interface facilitates reliable data exchange using a socket connection that utilizes the SCTP protocol (cf.\ref{fig:stack}), along with an Abstract Syntax Notation One (ASN.1) definition that structures the messages.
The use of ASN.1 ensures that the message formats are standardized, promoting interoperability and efficiency in data transmission.
This design allows the VM to effectively communicate with the xApp, enabling the integration of data extracted from video for the mobile RAN management.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\linewidth]{figures/VisionModule_ProtocolStack.drawio(2)}
    \caption[Proposed Vision Module Protocol Stack]{Proposed Vision Module Protocol Stack}
    \label{fig:stack}
\end{figure}

Since the mobility control of the RAN can be made through a xApp, as seen in \ref{Gonçalo}, we took this opportunity to approach the Vision Module interface to the E2 interface.
Due to time constrains, and since this was not one of the objectives of our work, we limited ourselves to
% continue...


\section{Proposed Vision Module}\label{sec:prop_vision}
Our Vision Module uses Computer Vision techniques to extract information about obstacles within the camera's field of view.
This system processes video frames to detect and track obstacles, subsequently sending messages to services connected with relevant information.

The module sends five types of messages: blockage, future blockage, past blockage, the location of the UE and frame processed.
Table~\ref{tab:message_type} summarizes the description of each message type.
Each message will be explained further in the following sections.

\begin{table}[H]
    \caption{Summary of each message type}
    \label{tab:message_type}
    \centering
    \resizebox{\textwidth}{!}{%
        \begin{tabular}{|l|p{12cm}|}
            \hline
            \textbf{Type of message} & \textbf{Description} \\ \hline
            Blockage Messages & Sent when an obstacle is currently blocking the UE. \\ \hline
            Future Blockage Messages & Sent when an obstacle is predicted to block the UE based on its current trajectory. \\ \hline
            Past Blockage Messages & Sent when an obstacle that was previously blocking the UE is no longer doing so. \\ \hline
            UE Location Messages & Provide the current location of the UE. \\ \hline
            Frame Processed & Provide information that a frame of the video was processed. \\ \hline
        \end{tabular}}
\end{table}

The module includes functions responsible for communication, detection and tracking, image processing, and utility functions.
We present a high level functionality overview of the developed Vision Module.
We utilized multi-threading in order to assure the timely processing of the video.
The first thread is responsible for handling communication, i.e.\ the connection with the xApp and sending the generated messages.
The second thread is responsible for the video processing, i.e.\ identifying objects and generating messages corresponding to the environment perception.

The processing of the video starts with the capture of frames from the camera.
One frame is processed at a time.
The UE has a ArUco Marker in order to identify its placement.
OpenCV is responsible for identifying this marker and returning the ROI corresponding to the marker, as well as its identification.
Periodically, the Module sends a message containing the location (normalized coordinates x and y) of the UE\@.
This is the first message designed, seeking to address XYZ. % finish this sentance
Following this, yolov8n, a YOLO pre trained model jointly with BoT-SORT, is responsible for detecting and tracking the obstacles in the frame.
The main returned value is a track history containing a unique identifier of the obstacle and its associated last positions (also normalized x, y, w and h coordinates), alongside its classes and confidence scores.

If it is noticed a difference between the last visible ArUcos and the current seen, the module checks if the obstacle last position intercepts the ArUcos position.
If this is true, the module generates a blockage message, indicating that an obstacle blocked the UE\@.

\textcolor{red}{CONTINUE EXPLANATION}


%It is worth considering that:
%\begin{itemize}
%    \item
%\end{itemize}

The file ObstacleDetectionReport.asn contains the specification of the set of messages created.
The messages are composed of a header and a payload.
The header of all messages is the same, as described in Table~\ref{tab:header}:


\begin{table}[H]
    \caption{Components of the Message Header}
    \label{tab:header}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            messageType & Identifies the type of the message. \\ \hline
            timestamp & Identify the  timestamp in which the message was created. \\ \hline
            sourceId & Identify the source of the message. In this case, the Vision Module.\\ \hline
            destinationId & Identifies the intended recipient of the message. In this case, the xApp. \\ \hline
            e2InstanceId & Provides a unique identifier for the E2 interface instance associated with the message. \\ \hline
        \end{tabular}
    }
\end{table}

As for the content of the payload, it varies according to the type of message.
Each message requires different processing in order to extract the information required, as mentioned previously.
The following subsections present further each message.

\subsection{Prediction of Blockage Messages}

To infer that an obstacle will block the line-of-sight (LOS) between the gNB and the UE, it is necessary to obtain a tracking history of the obstacle.
We achieved this by storing data over a number of frames, using the YOLO and BoT-SORT algorithms.
Once the tracking history is established, we have modeled the object's movement assuming a constant velocity.
This has proven effective for handling typical indoor movements, such as people walking or objects being moved.
By calculating the object's velocity, we can predict its future positions in upcoming video frames.
This allowed us to determine whether the obstacle will interrupt the LOS\@.
If the module predicts an impending blockage, it generates a message containing the information summarized in Table~\ref{tab:future_block_message}.


\begin{table}[H]
    \caption{Components of the Prediction of Blockage Payload}
    \label{tab:future_block_message}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            obstacleID & Unique identifier for the obstacle, provided by the tracking. \\ \hline
            obstacleType & Type of the obstacle detected. \\ \hline
            obstacleLocation & Location of the detected obstacle (x,y normalized coordinates, in the frame). \\ \hline
            obstacleVelocity & Velocity of the detected obstacle (normalized vector). \\ \hline
            obstacleConfidence & Confidence level in the identification of the object. \\ \hline
            timeToCross & Predicted time  that the obstacle will obstruct the UE. \\ \hline
            ueId & Identifier for the UE, in this case its ArUco identifier. \\ \hline
        \end{tabular}
    }
\end{table}



\textcolor{red}{Continue writing about other messages}
\subsection{Blocking messages}

\begin{table}[H]
    \caption{Components of the Prediction of Blockage Payload}
    \label{tab:block_payload}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            obstacleID & Unique identifier for the obstacle, provided by the tracking. \\ \hline
            obstacleType & Type of the obstacle detected. \\ \hline
            obstacleLocation & Location of the detected obstacle (x,y normalized coordinates, in the frame). \\ \hline
            obstacleConfidence & Confidence level in the identification of the object. \\ \hline
            timeBlocked & Time the obstacle has been blocking, up to 5000 milliseconds (optional). \\ \hline
            ueId & Identifier for the UE, in this case its ArUco identifier. \\ \hline
        \end{tabular}
    }
\end{table}


\subsection{Past Blockage}

\begin{table}[H]
    \caption{Components of the payload of PastBlockage Message}
    \label{tab:past_block_payload}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            obstacleID & Unique identifier for the obstacle, provided by the tracking. \\ \hline
            obstacleType & Type of the obstacle detected. \\ \hline
            obstacleLocation & Location of the detected obstacle (x,y normalized coordinates, in the frame). \\ \hline
            obstacleConfidence & Confidence level in the identification of the object. \\ \hline
            ueId & Identifier for the UE, in this case its ArUco identifier. \\ \hline
        \end{tabular}
    }
\end{table}

This message contains

\subsection{Location of UEs}

\begin{table}[H]
    \caption{Components of the UE Location Message payload}
    \label{tab:ue_payload}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            ueLocation & Contains the location of the UE (User Equipment). \\ \hline
            ueId & Identifier for the UE, ranging from 0 to 1023. \\ \hline
        \end{tabular}
    }
\end{table}


This message contains the location of the UEs. In order to extract such information it is required to detect the bounding boxes of the markers given for the UEs. 

The movement of the UE is monitored and sent when changes are detected.
This includes comparing current and previous visible ArUco IDs, buffering for changes, and sending location reports.

    ArUco IDs Comparison:
        Compare current and last visible ArUco IDs to detect changes.
    Buffering Changes:
        Use a time buffer to avoid rapid, repetitive changes.
    Sending Reports:
        Send location reports when the buffer is filled.

\subsection{Frame Processed}

\begin{table}[H]
    \caption{Components of the Frame Processed Message payload}
    \label{tab:frame_proc_pay}
    \centering
    \resizebox{\textwidth}{!}{
        \begin{tabular}{|c|c|}
            \hline
            \textbf{Field} & \textbf{Description} \\ \hline
            frameID & Sequential identification number of the frame \\ \hline
            timeProcessed & Time taken to process the frame in milliseconds \\ \hline
        \end{tabular}
    }
\end{table}



\section{System Design}\label{sec:design}

This section will present the system designed to implement and evaluate the proposed solution.
The system is depicted in Figure~\ref{fig:design_arch}.
It is composed of two main logical units.
The first implements the 5G Core Network, the Near-RT RIC and the Vision-aided gNB\@.
The second unit implements the UE software.


\begin{figure}
    \centering
    \includegraphics[width=0.7\linewidth]{figures/System Arch.drawio}
    \caption[System architecture designed for implementing and evaluating the proposed solution]{System architecture designed for implementing and evaluating the proposed solution}
    \label{fig:design_arch}
\end{figure}

The following subsections detail the hardware used in the implementation and the choices for software packages.


\subsection{Software}
This section presents the main software packages used to develop the Vision Module and implement the 5G network.
Beyond this, in the repository containing the VM, there is a file for reproducing the Python virtual environment.

\subsubsection{OpenCV}
% revision
There are several open-source software packages available for Computer Vision in Python.
Open Source Computer Vision Library (OpenCV) is one of them.
It contains several optimized algorithms, which can be used for a variety of tasks including object detection, image processing, and real-time video analysis.
This extensive algorithmic support makes it suitable for both basic tasks and complex applications requiring advanced functionalities.
Moreover, OpenCV's well-documented API facilitates ease of use and integration into various projects, ensuring efficient development and deployment.
Unlike some other libraries that may focus on specific aspects of image processing or lack comprehensive support across different domains, OpenCV provides a solution with cross-platform compatibility, making it applicable across diverse operating systems and hardware setups.
Furthermore, OpenCV benefits from a large and active community, offering extensive resources, tutorials, and community support, which are invaluable for developers and researchers seeking assistance or collaboration in tackling complex computer vision challenges effectively and efficiently.
Therefore, OpenCV is the leading choice for robust and scalable computer vision applications.



Additionally, OpenCV is the only library who directly supports ArUco Markers.
ArUco is a widely-used open-source library for Augmented Reality (AR) applications that involves detecting and tracking augmented markers.
These markers are specially designed square or rectangular patterns with a unique binary encoding, which can be printed and placed in the physical environment.
ArUco markers are typically used in computer vision tasks to provide reference points in a scene, enabling accurate localization and tracking of objects or cameras.

OpenCV emerges as the premier choice among general computer vision libraries, distinguished by its extensive feature set, optimized performance, and robust community support.
Unlike scikit-image, which excels in image processing tasks with functions for filtering, morphology, and segmentation, OpenCV offers a broader range of over 2500 optimized algorithms spanning image and video processing, object detection, and camera calibration.
In contrast to Pillow (PIL Fork), which specializes in image format handling and basic manipulation, OpenCV provides comprehensive tools for both foundational and advanced computer vision applications.
Moreover, while SimpleCV simplifies OpenCV usage with a user-friendly interface, OpenCV's C++ backend and GPU acceleration capabilities deliver superior performance crucial for real-time processing and large-scale data operations.
OpenCV's extensive documentation, tutorials, and active community support further solidify its position as the preferred choice for developers and researchers seeking versatility, efficiency, and collaborative resources in the realm of computer vision.

In our proposed solution, OpenCV is utilized to:
\begin{itemize}
    \item \textbf{Obtain Frames:} OpenCV is used to capture video frames from the camera. It provides easy-to-use interfaces to capture and manipulate video streams from various input sources.
    \item \textbf{Detect ArUco Markers:} OpenCV includes modules for detecting ArUco markers, which are widely used in computer vision applications for camera calibration, pose estimation, and augmented reality. In our solution, these markers help in identifying the UEs without the need to train the YOLO model to perceive such objects.
\end{itemize}

\subsubsection{Ultralytics YOLO}
As discussed in Section ~\ref{sec:CV}, Ultralytics YOLO (You Only Look Once) is a state-of-the-art, real-time object detection system.
It is known for its speed and accuracy, making it suitable for applications that require fast and reliable object detection and tracking.
While there are other solutions, also preseted in that Section, we chose Ultralytics YOLO due to simplicity, integration and since it is suitable for the intended application and use in state-of-the-art object detection and tracking situations.

In our proposed solution, Ultralytics YOLO is employed to:
\begin{itemize}
    \item \textbf{Detection:} YOLO is used to detect various objects in the frames captured by the camera. Its real-time capabilities allow for the immediate identification of obstacles within the field of view.
    \item \textbf{Tracking:} YOLO’s tracking module is used to keep track of detected objects over successive frames. This is crucial for maintaining the continuity of object identification and for predicting future positions of the obstacles. %BOTSORT and BYTETRACK
\end{itemize}

The combination of OpenCV and Ultralytics YOLO allows for robust detection, tracking, and communication functionalities in our vision module.
OpenCV handles the initial capture and processing of video frames, while YOLO and BoT-SORT performs the real-time detection and tracking of objects.
This integrated approach ensures that the vision module can effectively monitor and report on obstacles, providing key information to subscribed services.

\subsubsection{ASN1Tools and ASN1C}
There are a few open-source libraries available for handling ASN.1.
FlexRIC utilizes asn1c \cite{}, so for simplicity we chose the same for the xApp (client).
asn1c \cite[asn1c]{} is a open-source ASN.1 compiler that generated C/C++ code for ASN.1 data structures, supporting different encoding rules.
While this library also supports Python, we chose not to use it, since there are simpler APIs specially tailored for Python development.
As for the Vision Module (Python server), we chose ASN1Tools because it offers certain advantages.

ASN1Tools is a Python library provides a simple way to handle ASN.1 data structures, through a straightforward API\@.
The library support for a range of ASN.1 specifications and encoding rules, including BER, DER, and PER\@.
Moreover, ASN1Tools is actively maintained, with regular updates and improvements.
This guarantees that we have access to the latest features and bug fixes, which enhances the reliability and stability of our system.
ASN1Tools is optimized for performance, allowing for fast encoding and decoding operations.
This optimization is suited for real-time applications like our Vision Module, where processing speed is essential to maintain system responsiveness and accuracy.

While other libraries such as pyasn1 and libasn1 are available, they have certain limitations.
pyasn1, for instance, is less efficient in terms of performance and has a more complex API\@.
libasn1, part of the GNU project, is less user-friendly and optimized for performance.

\subsubsection{5G Core Network, 5G gNB, and 5G UE}
The main open-source 5G software packages for implementing an O-RAN based architecture are OAI \cite[]{} and srsRAN \cite[]{}.
For our system, we chose OAI because it provides all the necessary components to deploy a 5G standalone network, including both the RAN and Core networks.
In contrast, srsRAN only supports the deployment of the RAN, requiring an additional software package, such as OAI, to implement the Core network.

\subsubsection{Near-RT RIC}
The main open-source software packages able to implement a Near-RT RIC are Mosaic5G’s FlexRIC \cite[]{} and  O-RAN Software Community’s Near-RT RIC \cite[]{}.
The first was chosen more suitable for our application due to the fact that lightweight application, launched from an executable file.

\subsection{Hardware}
\subsubsection{Core Network, FlexRIC, Computer Vision module and gNB}
The OAI Core Network, FlexRIC,the Computer Vision module and the gNB were deployed in a laptop Acer Aspire A715-74G\@.
The OAI Core was deployed using Docker containers, requiring 4 cores CPU, 16GB of RAM and a minimum of 1.5GB free storage for the docker images.
As for FlexRIC, it does not have hardware requirements listed, but while deploying it was noticed that it is not resource intensive, sufficing the same ones required for the Core Network.
The xApp was deployed alongside FlexRIC in order to assure reduced latency between the two components.
Also,  the way interface E42 is implemented requires them to be running in the same computing unit.
As for the gNB, the hardware recommended are 8 physical CPU cores and 32 GB of RAM\@.
While the laptop used did not fulfill these requirements, it has proven sufficient to run the OAI gNB software, as it was noted before in \ref{}. % ref to dissertation of gonçalo

As for the Computer Vision Module, the minimal hardware requirement is a CUDA-compatible GPU ~\cite{}, for Ultralytics YOLO model to properly run.
Given that for our solution pre-trained model has proven enough to accurately detect and track objects, we did not worried about fulfilling the
%%%%%%%%%%%% finalize



Table ~\ref{tab:specs_pc} presents the specification of the computer.

\begin{table}[H]
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Specification} & \textbf{Details} \\ \hline
        Processor                      &           Intel(R) Core(TM) i5-9300H CPU @ 2.40GHz   \\ \hline
        RAM                      &          16GB        \\ \hline
        Disk                      &   2 SSDs  512GB and 256GB         \\ \hline
        GPU                     &   GeForce GTX 1050 (3GB)      \\ \hline
        Operational System & Ubuntu 22.04.4 LTS                  \\ \hline
    \end{tabular}\label{tab:specs_pc}
\end{table}

For the Computer Vision Module, we opted to use a webcam due to its simplicity and ease of integration.
The chosen webcam, a model LL-4196, offers Full HD (1920 x 1080 Pixels) resolution and supports a frame rate of 30 FPS\@.
This ensures that the video feed acquired is of high quality, providing sufficient detail and smooth motion necessary for accurate computer vision processing.
The camera connects to the computer using a USB 2.0 interface.
The specifications of the camera are sufficient for the system to detect and track movements and objects within the environment, assuring the efficient operation of the Computer Vision Module.
% why? velocity constant , processing of frame


Deploying the gNB and the UE in two different host computers implicates the use of Software-Defined Radio (SDR) in order to establish a 5G connection between the gNB and the UE.
OAI recommends the use of three SDR models: USRP B210, USRP N300 and USRP X300\@ \cite{}. %https://gitlab.eurecom.fr/oai/openairinterface5g/-/blob/develop/doc/NR_SA_Tutorial_OAI_nrUE.md
For our implementation, we have selected the first since it is cost-effective and a popularity across the community.
This model uses a USB 3.0 interface to connect to the computer acting as a processing unit.
The SDR was equipped with two W5208K dipole antennas.
Figure \ref{} shows the two SDRs with their respective antennas.
We have selected 3.6GHz as the carrier frequency for the 5G RAN.

\subsubsection{UE}
The UE was deployed in HP EliteBook 840 Laptop.
The hardware requirements for the deployment of the UE are 8 cores and 8GB of RAM\@.
Table~\ref{tab:specs_pc_ue} depicts the specification of the computer responsible for the UE\@.

\begin{table}[H]
    \begin{tabular}{|c|c|}
        \hline
        \textbf{Specification} & \textbf{Details} \\ \hline
        Processor                      &              \\ \hline
        RAM                      &          8GB        \\ \hline
        Disk                      &   2 SSDs  512GB and 256GB         \\ \hline
        GPU                     &   None                \\ \hline
        Operational System & Ubuntu 22.04.4 LTS                  \\ \hline  %  check
    \end{tabular}\label{tab:specs_pc_ue}
\end{table}
\textcolor{red}{Finish table!!!!}

\section{System Implementation}\label{sec:impl}
\textcolor{red}{Improve this paragraph}
For the Vision Module:

    Coding:
        Implementing object detection and tracking using YOLO and BoT-SORT algorithms.
        Implementing ArUco marker detection using OpenCV
        Developing the communication interface using SCTP and ASN.1 for message encoding.
        Handle video input, process each frame, and generate output messages.

    Testing:
        Running unit tests to ensure each component (detection, tracking, messaging) works as intended.
        Conducting integration tests to verify that components interact correctly.
        Performing system tests with both pre-recorded videos and real-time captures to ensure the module meets the specified performance criteria (e.g., processing speed, accuracy).

    Deployment:
        Deploying the vision module on the specified hardware.
        Monitoring performance in real-time scenarios to ensure it meets the defined requirements.
        Making adjustments and optimizations based on observed performance and feedback.

\subsection{OAI 5G Core Network}
OAI offers three methods for implementing the Core Network: bare-metal installation or virtual machines, automated deployment of network functions (NFs) in Docker containers using Docker-Compose, and cloud-native deployment using Helm Chart on OpenShift or Kubernetes clusters ~\cite{}.
Choosing Docker for deployment simplifies the process by encapsulating network functions in containers, making them easier to manage, scale, and automate, which enhances the overall efficiency and flexibility of the network infrastructure.

Besides that, the Core can be implemented in two scenarios.
We chose scenario I , since it contains the minimum components necessary in order to test end to end connectivity between the nodes.
This scenario contains 8 containers.
python3 core-network.py --type start-basic --scenario 1
- Scenario I:  AMF, SMF, UPF (SPGWU), NRF, UDM, UDR, AUSF, MYSQL

\textcolor{red}{Finish explanation}

\textcolor{red}{Finish writing the other subsections}
\subsection{OAI gNB}
\subsection{OAI 5G UE}
\subsection{FlexRIC}

% what else?

\section{Summary}






